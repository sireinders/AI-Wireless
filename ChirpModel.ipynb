{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d7dcc7-4402-46a7-a66d-51d46bd083ef",
   "metadata": {},
   "source": [
    "### Import Pytorch Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d46ec355-5da7-4646-a561-d9ef1475ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import scipy.signal as sps\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2636abb-fc96-43b0-b992-3bf6de111ac0",
   "metadata": {},
   "source": [
    "### Feature Extraction from .wav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dab26337-9ba4-4d30-91a5-be37ae8bb590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(filepath):\n",
    "    x, fs = sf.read(filepath)\n",
    "\n",
    "    # Recreate chirp\n",
    "    f0, f1, T = 15000, 20000, 0.012 # freq sweep, chirp duration\n",
    "    t = np.linspace(0, T, int(fs*T), endpoint=False)\n",
    "    chirp = sps.chirp(t, f0, f1, T, method='linear')\n",
    "\n",
    "    # Matched filter to clean signal and convert to impulse response\n",
    "    ir = sps.fftconvolve(x, chirp[::-1], mode='same')\n",
    "    ir = np.abs(ir)\n",
    "\n",
    "    # Find primary reflection (eliminate weaker multi-path)\n",
    "    peak = np.argmax(ir)\n",
    "    win = ir[max(0, peak-300):peak+600]  # define window to locate peak amplitude within ir\n",
    "\n",
    "    # FFT magnitude\n",
    "    X = np.abs(np.fft.rfft(win * np.hanning(len(win))))\n",
    "    freqs = np.fft.rfftfreq(len(win), 1/fs)\n",
    "\n",
    "    # Band energies helpful for material ID\n",
    "    bands = [(2e3,5e3), (5e3,10e3), (10e3,18e3)]\n",
    "    feats = [np.mean(X[(freqs>=lo)&(freqs<hi)]) for lo,hi in bands]\n",
    "\n",
    "    # Spectral centroid\n",
    "    centroid = np.sum(freqs * X) / np.sum(X)\n",
    "    feats.append(centroid)\n",
    "\n",
    "    return np.log1p(np.array(feats, dtype=np.float32)) # (4,)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "819ec934-093e-4770-af62-3e630968abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset class\n",
    "\n",
    "class ChirpDataset(Dataset):\n",
    "    def __init__(self, csv_path, root):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.root = root\n",
    "\n",
    "        # Encode material â†’ integer\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.df[\"material_idx\"] = self.label_encoder.fit_transform(self.df[\"material\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Load waveform + extract features\n",
    "        wav_path = os.path.join(self.root, row[\"filename\"])\n",
    "        feats = extract_features(wav_path)\n",
    "\n",
    "        material = row[\"material_idx\"]\n",
    "        dist = row[\"distance_m\"]\n",
    "\n",
    "        return (\n",
    "            torch.tensor(feats, dtype=torch.float32), # X\n",
    "            torch.tensor(material, dtype=torch.long), # material label\n",
    "            torch.tensor([dist], dtype=torch.float32) # distance\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbe63e37-2be6-48a1-96ec-b3d3fffcf548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "\n",
    "class EchoNet(nn.Module):\n",
    "    def __init__(self, in_dim=4, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.classifier = nn.Linear(16, num_classes) # material classification\n",
    "        self.regressor = nn.Linear(16, 1) # distance regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.classifier(x), self.regressor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e41ef6d-5b2c-49be-a7f0-f7a258c0be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "def train(dataset_folder=\"dataset\"):\n",
    "\n",
    "    csv_path = os.path.join(dataset_folder, \"chirp_data.csv\")\n",
    "    ds = ChirpDataset(csv_path, dataset_folder)\n",
    "    loader = DataLoader(ds, batch_size=16, shuffle=True)\n",
    "\n",
    "    num_classes = len(ds.label_encoder.classes_)\n",
    "    model = EchoNet(in_dim=4, num_classes=num_classes)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(50):\n",
    "        total_cls = 0.0\n",
    "        total_reg = 0.0\n",
    "\n",
    "        for feats, mat, dist in loader:\n",
    "            opt.zero_grad()\n",
    "\n",
    "            logits, pred_dist = model(feats)\n",
    "\n",
    "            loss_cls = F.cross_entropy(logits, mat)\n",
    "            loss_reg = F.mse_loss(pred_dist.squeeze(), dist.squeeze())\n",
    "\n",
    "            loss = loss_cls + 0.1 * loss_reg # weight regression lightly\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_cls += loss_cls.item()\n",
    "            total_reg += loss_reg.item()\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | cls={total_cls:.3f} | reg={total_reg:.3f}\")\n",
    "\n",
    "    # Save PyTorch model\n",
    "    torch.save({\n",
    "        \"model\": model.state_dict(),\n",
    "        \"classes\": ds.label_encoder.classes_\n",
    "    }, \"echo_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf29ad81-3aad-4f54-8a79-c40d2f6943b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model on unseen data\n",
    " example = torch.rand(1, 4)\n",
    "    model.eval()\n",
    "    traced = torch.jit.trace(model, example)\n",
    "    traced.save(\"echo_model_mobile.pt\")\n",
    "\n",
    "    print(\"\\nSaved:\")\n",
    "    print(\" - echo_model.pth (PyTorch)\")\n",
    "    print(\" - echo_model_mobile.pt (TorchScript for Android)\")\n",
    "    print(\"\\nClasses:\", list(ds.label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef89fd6d-20dc-4282-ad99-b0bfcf0f4f75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
